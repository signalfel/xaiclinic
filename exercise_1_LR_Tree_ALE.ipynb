{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install dtreeviz"
      ],
      "metadata": {
        "id": "2-Gevied1RRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Lqit1Ui_yw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "eljest = {'natt': '#0E171B',\n",
        "          #'kotte': '#422A25',\n",
        "          'kotte': '#4E322C',\n",
        "          'barr': '#50933B',\n",
        "          'skymning': '#3B7C93',\n",
        "          'gryning': '#BD6D24'}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature information\n",
        "\n",
        "\n",
        "#### Target\n",
        "- total_fuel - Total fuel consumed in litres during the trip. Calculated from the fuel rate data in the raw VED data.\n",
        "\n",
        "### Features\n",
        "- total_time - Total duration in seconds of the trip, calculated from the tracking timestamp\n",
        "\n",
        "- total_distance - Total driven distance in km of the trip, calculated as the sum of the $\\Delta$Haversine distances between lat, lon points in the tracking data\n",
        "\n",
        "- avg_speed - Average speed in km/h during the trip, calculated as a mean of the wheel-based speed signal\n",
        "\n",
        "- std_speed - Standard deviation of all wheel-based speed readings in kmph during the trip\n",
        "\n",
        "- max_speed - Maximum wheel-based speed reading during the trip\n",
        "\n",
        "- min_acc - Minimum acceleration during the trip in m/s$^{-2}$\n",
        "\n",
        "- max_acc - Maximum acceleration during the trip in m/s$^{-2}$\n",
        "\n",
        "- std_acc - Standard deviation of the acceleration signal during the trip in m/s$^{-2}$, trying to capture jittery driving.\n",
        "\n",
        "- n_harsh_accelerations - Number of times during the trip that the acceleration signal has exceeded 5 m/s$^{-2}$\n",
        "\n",
        "- hour_of_day - The most common hour value among all the timestamps in the trip trajectory\n",
        "\n",
        "- generalized_weight - Estimated mass of the vehicle, taken from the static raw VED data.\n",
        "\n",
        "- engine_volume - The engine volume in litres of the vehicle, regexed from the engine type string in the static raw VED data.\n",
        "\n",
        "Note: *The acceleration signal was calculated by numerical differentiation of the wheel-based speed w.r.t. time. The typical $\\Delta t$ of the raw data is slightly less than 1 sec. Both the velocity signal and the resulting acceleration signal have been treated with a moving average of 3 periods to reduce the noise somewhat. But the acceleration signal is still very spiky and unrealstic and the whole calculation would ideally need to be filtered more to be realistic.*\n"
      ],
      "metadata": {
        "id": "zzrVa-Be4rWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/signalfel/xaiclinic/main/ved_ice_trips.csv')"
      ],
      "metadata": {
        "id": "WAXMstIMjQuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 1:]\n",
        "y = data.total_fuel"
      ],
      "metadata": {
        "id": "pE3-nL-PlVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "1_QZRDOmlb9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "GkU-v7-Zle3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split. Everyone uses the same random_state so that we\n",
        "# can easily compare results\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1337)"
      ],
      "metadata": {
        "id": "5iUUFYhIlhcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1 - Linear Regression**"
      ],
      "metadata": {
        "id": "quFE3K6bwZpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Linear Regression model"
      ],
      "metadata": {
        "id": "nNDhXf_mmJF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Train the model on the training set (Note that we don't\n",
        "# perform any feature scaling, such as removing the mean and forcing unit std)\n",
        "lr.fit(X_train, y_train);\n",
        "\n",
        "# Print the R2 score for the test set (should be 0.86)\n",
        "print(r2_score(lr.predict(X_test), y_test).round(3))"
      ],
      "metadata": {
        "id": "xQUvj4hnlsYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ What do the weights in a linear model tell us?"
      ],
      "metadata": {
        "id": "Hmi8XVq0ngWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "vWzbGxaTQ4va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's calculate and plot the weights"
      ],
      "metadata": {
        "id": "wXb6P1f3UbJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = lr.coef_\n",
        "df_weights = pd.DataFrame({'weight': weights}, index=X_train.columns)"
      ],
      "metadata": {
        "id": "ft2Yx6PPmMn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weights.plot(kind='barh', color=eljest['kotte'])"
      ],
      "metadata": {
        "id": "8zvj9yoWnoA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ Create an effect plot from the weights"
      ],
      "metadata": {
        "id": "BwEbdDQiuKGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effects of a feature on a single data instance $i$ is calculated as\n",
        "\n",
        "\\begin{equation}\n",
        "E_j^{(i)} = w_j x_j^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "where $w_j$ is the weight of feature $j$ and $x_j^{(i)}$ is the value of the feature $j$ for data instance $i$.\n",
        "\n",
        "There will of course be as many effects as there are data instances, so we need to figure out how to visualize it."
      ],
      "metadata": {
        "id": "Y48lnvyNxy4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effects = ..."
      ],
      "metadata": {
        "id": "xhxNcj62v8N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ What does the Effect plot tell us that the weights do not?"
      ],
      "metadata": {
        "id": "batxjLq9v3cw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "0-5G6k3tv5HF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - Understanding tree models**\n",
        "\n"
      ],
      "metadata": {
        "id": "-Ta_zYglxHgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# We start with a very shallow tree to be able to visualize it\n",
        "tree_depth = 3\n",
        "\n",
        "dtree = DecisionTreeRegressor(criterion='squared_error',\n",
        "                                    max_depth=tree_depth,\n",
        "                                    random_state=1337)\n",
        "\n",
        "\n",
        "# Again, no feature scaling\n",
        "dtree.fit(X_train, y_train);\n",
        "\n",
        "# Print the R2 score for this very shallow tree, depth = 3 (should be 0.694)\n",
        "print(r2_score(dtree.predict(X_test), y_test).round(3))"
      ],
      "metadata": {
        "id": "4zHNnRJwxLWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### graphviz"
      ],
      "metadata": {
        "id": "HL8gnssWIDX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "g = tree.export_graphviz(dtree,\n",
        "                         feature_names=X_train.columns.tolist(),\n",
        "                         filled=True)\n",
        "graphviz.Source(g)"
      ],
      "metadata": {
        "id": "pTeicPr2z-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dtreeviz"
      ],
      "metadata": {
        "id": "PX-wta4D2PF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dtreeviz\n",
        "\n",
        "viz = dtreeviz.model(dtree, X_train, y_train,\n",
        "                     target_name='total_fuel',\n",
        "                     feature_names=X_train.columns)\n",
        "viz.view(fontname='monospace')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gAw83_n_0MYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ Which of the two do you prefer? What are some pros and cons of either?"
      ],
      "metadata": {
        "id": "IjKQ_27oJ_PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "5dEU7XHIQ0JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ Let's investigate how the performance varies when we make the tree deeper. Finish the for loop (sorry, not very engaging, but hey)"
      ],
      "metadata": {
        "id": "HlZ8pwvVKjxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2_scores = []\n",
        "tree_depths = list(range(2, 16))\n",
        "random_state = 1337\n",
        "\n",
        "for tree_depth in tree_depths:\n",
        "  ..."
      ],
      "metadata": {
        "id": "IrMgTzfrL68S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "plt.plot(tree_depths, r2_scores, color=eljest['barr'], marker='o')\n",
        "plt.ylim([0.6, 0.9])\n",
        "plt.xlim([tree_depths[0], tree_depths[-1]])\n",
        "plt.xticks(tree_depths);\n",
        "\n",
        "plt.legend(['R2 score'])\n",
        "plt.xlabel('Tree depth');\n",
        "plt.ylabel('Score');\n",
        "plt.title('Model performance');"
      ],
      "metadata": {
        "id": "uxOxB-IKMJdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ How feasible is it to use the above methods to explain a tree that performs well? Try it out :)"
      ],
      "metadata": {
        "id": "NXOU3BHzM7Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iS8AbAbHQvLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 - Accumulated Local Effects (ALE)**"
      ],
      "metadata": {
        "id": "QJkKfODv9ezN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented some inefficient code to calculate ALE so you can get a bit clearer view\n",
        "how it's done (Hopefully I understood it correctly?). Completely optional to study the code of course."
      ],
      "metadata": {
        "id": "JffdFBT2Nek7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ALE(feature, model, X, n_neighbourhoods = 20):\n",
        "\n",
        "  ALE_df = X.copy()\n",
        "\n",
        "  # Define the neighbourhoods, in this case <n_neighbourhoods> evenly spaced\n",
        "  # slices that span the 1-D feature space for the selected feature\n",
        "  feature_neighbourhoods = np.linspace(X[feature].min(),\n",
        "                                          X[feature].max(),\n",
        "                                          n_neighbourhoods)\n",
        "\n",
        "  # Split the feature of interest into the slices\n",
        "  ALE_df['interval'] = pd.cut(X[feature],\n",
        "                              feature_neighbourhoods,\n",
        "                              include_lowest=True)\n",
        "\n",
        "  ale_vector = []\n",
        "  n_samples_vector = []\n",
        "\n",
        "  # For clarity, we loop through the slices\n",
        "  for g in ALE_df.groupby('interval'):\n",
        "\n",
        "    feature_interval = g[0]\n",
        "    X_in_interval = g[1]\n",
        "\n",
        "    # If we don't have any data instances in the interval,\n",
        "    # we return nan and skip the calculation\n",
        "    if (n_samples_in_nei := X_in_interval.shape[0]) == 0:\n",
        "      ale_vector.append(np.nan)\n",
        "      continue\n",
        "\n",
        "    # We calculate the feature value at the left and\n",
        "    # at the right of the neighbourhood\n",
        "    z_left = feature_interval.left\n",
        "    z_right = feature_interval.right\n",
        "\n",
        "    # We extract the X_train data for the slice (g) at hand\n",
        "    # and replace all values of <feature> with the rightmost\n",
        "    # edge of the neighbourhood. All the other features of the\n",
        "    # data instances are kept unchanged\n",
        "    X_right = X_in_interval.copy()[X_train.columns]\n",
        "    X_right[feature] = z_right\n",
        "\n",
        "    # We do the same for the left edge of the neighbourhood\n",
        "    X_left = X_in_interval.copy()[X_train.columns]\n",
        "    X_left[feature] = z_left\n",
        "\n",
        "    # We get the model predictions for each of the modified data instances\n",
        "    # and get their difference.\n",
        "    # We use the model we were passed to do the predictions. It of course\n",
        "    # assumes the model has implemented a .predict()-method\n",
        "    y_pred_right = model.predict(X_right)\n",
        "    y_pred_left = model.predict(X_left)\n",
        "    delta_y = (y_pred_right - y_pred_left)\n",
        "\n",
        "    # The local effect for this neighbourhood is then the sum of the\n",
        "    # differences in predictions divided by the number of samples\n",
        "    # (i.e. the average difference of predictions in this interval)\n",
        "    ale_vector.append(round((delta_y.sum() / n_samples_in_nei), 3))\n",
        "    n_samples_vector.append(n_samples_in_nei)\n",
        "  # return the neighbourhood vector and the accumulated local effects\n",
        "  return feature_neighbourhoods[:-1], ale_vector, n_samples_vector"
      ],
      "metadata": {
        "id": "hUPzDepHGvkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train a quick XGBoost regressor that we can study, hyperparameter tuning is not important for this purpose"
      ],
      "metadata": {
        "id": "C-GSrwc_Qmsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb = xgboost.XGBRegressor(n_estimators=300,\n",
        "                           max_depth=2,\n",
        "                           random_state=1337)\n",
        "xgb.fit(X_train, y_train);\n",
        "\n",
        "print(r2_score(xgb.predict(X_test), y_test).round(3))"
      ],
      "metadata": {
        "id": "MCCzx-Y9O5dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ Use the function above or any other method you choose to calculate the ALE of the model xgb, for the feature \"total_time\". Plot the ALE."
      ],
      "metadata": {
        "id": "2ba6AWNqROAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_neighbourhoods, ale_vector, n_samples_vector = ..."
      ],
      "metadata": {
        "id": "85veTAh5RpkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO❗ Explain what the ALE plot shows"
      ],
      "metadata": {
        "id": "OlDSIYwMQ6YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "BYw9JpamRsf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTIONAL TODO❓ Plot a line plot of the n_samples_vector (or like a bar plot to mimic a histogram of the samples)\n",
        "This histogram would perform the same function as the rug plot in the ALE example plot in the PPT (and Molnar's book). What does it tell us? Does it help us interpret the ALE plot?"
      ],
      "metadata": {
        "id": "l8mnA6xISniA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iC251cxmU8SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTIONAL TODO❓ What would happen if we calculated the ALE for the LinearRegressor we trained in Part 1?\n",
        "\n",
        "**We encourage you to first think about it and then plot it!**"
      ],
      "metadata": {
        "id": "HjjyCjPXRbN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "CXJZbiOwRvc-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P51LnL-ZU6V_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}