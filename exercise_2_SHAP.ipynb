{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install flaml[automl]"
      ],
      "metadata": {
        "id": "j2Z_9mHMkG2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install shap"
      ],
      "metadata": {
        "id": "mDBUZpTNvqYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Lqit1Ui_yw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from flaml import AutoML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost\n",
        "import shap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we load some extra features"
      ],
      "metadata": {
        "id": "0U0JDzOlF2PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/signalfel/xaiclinic/main/ved_ice_trips_extra_features.csv')"
      ],
      "metadata": {
        "id": "WAXMstIMjQuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 1:]\n",
        "y = data.total_fuel"
      ],
      "metadata": {
        "id": "pE3-nL-PlVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "1_QZRDOmlb9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "GkU-v7-Zle3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split. Everyone uses the same random_state so that we\n",
        "# can easily compare results\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1337)"
      ],
      "metadata": {
        "id": "5iUUFYhIlhcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoML (from flaml)"
      ],
      "metadata": {
        "id": "XZhYJVhpsFlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use AutoML to select a model for our dataset.\n",
        "\n",
        "For those of you unaware, AutoML will train a bunch of different models and do some hyperparameter tuning automatically. All you have to do is select the type of problem (regression here), loss metric (R2 here) and the time in seconds you want the total training of all models to take. AutoML will do it's best with that time.   "
      ],
      "metadata": {
        "id": "CvLsEDEtuI6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an AutoML instance\n",
        "automl = AutoML()\n",
        "# Specify automl goal and constraint\n",
        "automl_settings = {\n",
        "    \"time_budget\": 60,  # in seconds\n",
        "    \"metric\": \"r2\",\n",
        "    \"task\": \"regression\",\n",
        "    \"log_file_name\": \"automl.log\",\n",
        "}\n",
        "\n",
        "# Train with labeled input data\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oBeFHajRsE-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best model\n",
        "print(automl.model.estimator)\n",
        "\n",
        "# Performance of best model\n",
        "print(r2_score(y_test, automl.model.predict(X_test)).round(3))"
      ],
      "metadata": {
        "id": "iG84oE0WtBzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: We don't care what ended up being the best model chosen by the AutoML. This is one of the good things about SHAP! We don't have to! It's model agnostic! üéâ"
      ],
      "metadata": {
        "id": "fOp0H8W4diPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1 - Global feature importance**"
      ],
      "metadata": {
        "id": "quFE3K6bwZpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The PermutationExplainer  approximates the Shapley values by iterating through\n",
        "# permutations of the inputs.\n",
        "# The arguments are:\n",
        "# 1. the model in question\n",
        "# 2. a masker, i.e. a feature matrix from which to get other values to permute\n",
        "#   among when studying a given feature. Here we choose to use the whole dataset\n",
        "#   to allow for maximum variability. Some argue that only using the test set\n",
        "#   makes more sense.\n",
        "# 3. the names of the features\n",
        "permutation_explainer = shap.PermutationExplainer(automl.model.predict, X,\n",
        "                                      feature_names=X.columns)"
      ],
      "metadata": {
        "id": "mdfFdrDowDvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We select only a small subset of the test set to calculate the SHAP values\n",
        "# since it takes quite a long time to calculate the permutations\n",
        "\n",
        "# Note: we take .head(500) instead of .sample(500) so that it's comparable\n",
        "# between one another (and X_test is already scrambled)\n",
        "shaps = permutation_explainer(X_test.head(500))"
      ],
      "metadata": {
        "id": "MaXosjx8wSuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shaps, feature_names=X.columns)"
      ],
      "metadata": {
        "id": "EcIOIYv8E4Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shaps)"
      ],
      "metadata": {
        "id": "cGFLezkWGnBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó What does this plot tell us? Which features are the most important ones? Which could maybe be left out?"
      ],
      "metadata": {
        "id": "vgGXTX4hANIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BuwpZAItJJFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - Feature pruning and re-training**"
      ],
      "metadata": {
        "id": "xiSVenKTntx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SHAP values can be harder to interpret if there are many correlated features.\n",
        "This is because the feature importance tends to be spread out among those correlated features."
      ],
      "metadata": {
        "id": "284V6KZKI0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = X.corr()\n",
        "sns.heatmap(corr,\n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values)"
      ],
      "metadata": {
        "id": "l6SrppahGwVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do a hierarchical clustering of the features relative to the target variable, y.\n",
        "Simply put, it is a way to estimate how much predictive power is added by including the feature.\n"
      ],
      "metadata": {
        "id": "wQbGimreMMVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = shap.utils.hclust(X_train, y_train);"
      ],
      "metadata": {
        "id": "e529vuvjHFFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster import hierarchy\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "hierarchy.dendrogram(clustering, labels=X.columns, orientation='right');\n",
        "plt.xlabel('Cluster distance (loosely 1 - corr)');"
      ],
      "metadata": {
        "id": "ekPlJUliHKPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: The hierarchical clustering says quite little about the *actual* predictive power of the feature, as illustrated by the fact that hour_of_day is quite high up in the list in the dendrogram above (c.f. the global feature importance of that feature).\n",
        "\n",
        "Rather, this method relates the features to one another.\n",
        "Something more like: \"we might not need **n_points**, since whatever predictive power it holds is also captured by **n_harsh_brakes**\"."
      ],
      "metadata": {
        "id": "cVLdVZkfNQf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot a SHAP bar plot using the information from the hierarchical clustering like so:"
      ],
      "metadata": {
        "id": "Mh19aLKjN7wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shaps, clustering=clustering, clustering_cutoff=0.6,\n",
        "               max_display=8)"
      ],
      "metadata": {
        "id": "iwsKATg5HxeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó Remove at least 4 features and train a new model. Choose based on both the global feature importances and the information you can gain from the correlation study above."
      ],
      "metadata": {
        "id": "U4-fWB7VJ1TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_drop = []\n",
        "\n",
        "X_train_slimmed = X_train.drop(columns=features_to_drop)\n",
        "X_test_slimmed = X_test.drop(columns=features_to_drop)\n",
        "X_slimmed = X.drop(columns=features_to_drop)"
      ],
      "metadata": {
        "id": "gGc23PDy_dd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl_slimmed = AutoML()\n",
        "\n",
        "automl_slimmed.fit(X_train=X_train_slimmed,\n",
        "                   y_train=y_train,\n",
        "                   **automl_settings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PXJbz9OMKQVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best model\n",
        "print(automl_slimmed.model.estimator)\n",
        "\n",
        "# Performance of best model\n",
        "print(r2_score(y_test, automl_slimmed.model.predict(X_test_slimmed)).round(3))"
      ],
      "metadata": {
        "id": "iLZH_5i_OrSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó How is the R2-score impacted? Are you able to tell if any of the previously unimportant features are more important now due to pruning correlated features?"
      ],
      "metadata": {
        "id": "uwOc-tBaWL36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "permutation_explainer_slimmed = shap.PermutationExplainer(\n",
        "              automl_slimmed.model.predict, X_slimmed,\n",
        "              feature_names=X_slimmed.columns)"
      ],
      "metadata": {
        "id": "WcYNjYa3WUas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shaps_slimmed = permutation_explainer_slimmed(X_test_slimmed.head(500))"
      ],
      "metadata": {
        "id": "acxESth2Wfvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shaps_slimmed, feature_names=X_slimmed.columns)"
      ],
      "metadata": {
        "id": "zHOOmRp_lc1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 - Understanding individual predictions**"
      ],
      "metadata": {
        "id": "YvldVutOAltg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó Find two data instances that have quite different SHAP values and look at the waterfall plots"
      ],
      "metadata": {
        "id": "TT75b_KTmlLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_1 = ...\n",
        "index_2 = ..."
      ],
      "metadata": {
        "id": "Dp_XRkej-Trd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.waterfall_plot(shaps[index_1])"
      ],
      "metadata": {
        "id": "thc4nUrr-iDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.waterfall_plot(shaps[index_2])"
      ],
      "metadata": {
        "id": "-xalynmd-4Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó What is the anatomy of this waterfall plot? In what way is it explaining this particular prediction?"
      ],
      "metadata": {
        "id": "NR-MMkPJbX0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "NIU7lm43bnLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó Can you think of any use for the SHAP values other than understanding the model?"
      ],
      "metadata": {
        "id": "IunetLVBcVyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "ei54p7zlcida"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4 - Deep-dive on a feature**"
      ],
      "metadata": {
        "id": "GAUPQdIkoLgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO‚ùó Use shap.plots.scatter to look at all the SHAP values of a feature.\n",
        "\n",
        "If you want to color the dots by the value of another feature to look at how those features interact,\n",
        "use color=shaps[:, feature]"
      ],
      "metadata": {
        "id": "cDC88JPuEeMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.scatter(shaps[:, '<...>'])\n",
        "\n",
        "# or\n",
        "\n",
        "# shap.plots.scatter(shaps[:, '<...>'], color = shaps[:, '<...>'])"
      ],
      "metadata": {
        "id": "osZh6d0T_jU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quick Bonus demo: TreeShap**"
      ],
      "metadata": {
        "id": "e4gKZ2Dy76hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just train an XGBoost so that we can try out the TreeExplainer. Mostly just for you to see the big difference in the time it takes to calculate the SHAP values!  "
      ],
      "metadata": {
        "id": "RElqxmSd0fyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = xgboost.XGBRegressor(n_estimators=500,\n",
        "                           max_depth=4,\n",
        "                           min_child_weight=2,\n",
        "                           random_state=411)\n",
        "xgb.fit(X_train, y_train);\n",
        "\n",
        "print(r2_score(xgb.predict(X_test), y_test).round(3))"
      ],
      "metadata": {
        "id": "6URSwPnoyg5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_explainer = shap.TreeExplainer(xgb, X,\n",
        "                               feature_names=X.columns)"
      ],
      "metadata": {
        "id": "VUa3U85cxTGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treeshaps = tree_explainer(X_test.head(500))"
      ],
      "metadata": {
        "id": "7eMdc20Q00A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(treeshaps, feature_names=X.columns)"
      ],
      "metadata": {
        "id": "YJq9LuQ67zEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}